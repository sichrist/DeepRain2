{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 1\n",
      "Num GPUs Available: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from Utils.loadset import getDataSet\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "print(\"Num GPUs:\", len(physical_devices)) \n",
    "\n",
    "from trainer import Trainer\n",
    "try:\n",
    "    from Utils.connection_cfg import *\n",
    "except Exception as e:\n",
    "    PSWD = None\n",
    "    USRN = None\n",
    "    \n",
    "from Utils.Data import dataWrapper, provideData\n",
    "from Utils.transform import cutOut\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "def NLL(y_true, y_hat):\n",
    "    return -y_hat.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mFound Year \u001b[0m:  2017 => won't download this year again... please check for consistency\n",
      "\u001b[32mFinished Loading Dataset\n",
      " \u001b[0m\n",
      "[DEBUG] Data\n",
      "[DEBUG] time to predict:  5\n",
      "SORTTING OUUUT 61997\n",
      "[DEBUG] Data\n",
      "[DEBUG] time to predict:  5\n",
      "SORTTING OUUUT 22305\n"
     ]
    }
   ],
   "source": [
    "dimension = (512,512)\n",
    "\n",
    "channels = 5\n",
    "\n",
    "slices = [256,256+512,256,256+512]\n",
    "slices_label = [128,128+256,128,128+256]\n",
    "cutOutFrame = cutOut(slices)\n",
    "cutOutFrame_label = cutOut(slices_label)\n",
    "years = [2017]\n",
    "PRETRAINING_TRANSFORMATIONS = [cutOutFrame]\n",
    "TRANSFORMATION = [cutOutFrame_label]\n",
    "batch_size = 25\n",
    "\n",
    "train, test = provideData(dimension=dimension,\n",
    "                          batch_size=batch_size,\n",
    "                          channels = channels,\n",
    "                          timeToPred = 5,\n",
    "                          transform=TRANSFORMATION,\n",
    "                          onlyUseYears=[2017],\n",
    "                          preTransformation=PRETRAINING_TRANSFORMATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def negative_Binomial_Unet_BigBrainTime(input_shape,\n",
    "                           activation_hidden=\"relu\",\n",
    "                           activation_output=\"relu\"):\n",
    "    \n",
    "    inputs = Input(shape=input_shape) \n",
    "\n",
    "    conv00 = Conv2D(5, kernel_size=(5, 5), dilation_rate=3,padding=\"same\")(inputs)\n",
    "    conv00 = Activation(activation_hidden)(conv00)\n",
    "    conv00 = Conv2D(10, kernel_size=(5, 5), strides=4,padding=\"same\")(conv00)\n",
    "    conv00 = Activation(activation_hidden)(conv00)\n",
    "    \n",
    "    \n",
    "    conv01 = Conv2D(5, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(conv00)       \n",
    "    conv01 = Activation(activation_hidden)(conv01)\n",
    "    \n",
    "    \n",
    "    dilated_01 = Conv2D(5, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(conv00)\n",
    "    dilated_01 = Activation(activation_hidden)(dilated_01)\n",
    "    dilated_01 = Conv2D(5, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_01)       \n",
    "    dilated_01 = Activation(activation_hidden)(dilated_01)\n",
    "    \n",
    "    concatenate_01 = concatenate([conv01, dilated_01], axis=3)\n",
    "    \n",
    "    conv02 = Conv2D(10, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(concatenate_01)       \n",
    "    conv02 = Activation(activation_hidden)(conv02)\n",
    "    \n",
    "    dilated_02 = Conv2D(10, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(concatenate_01)\n",
    "    dilated_02 = Activation(activation_hidden)(dilated_02)\n",
    "    dilated_02 = Conv2D(10, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_02)       \n",
    "    dilated_02 = Activation(activation_hidden)(dilated_02)\n",
    "    \n",
    "    concatenate_02 = concatenate([conv02, dilated_02], axis=3)\n",
    "    \n",
    "    conv03 = Conv2D(10, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(concatenate_02)       \n",
    "    conv03 = Activation(activation_hidden)(conv03)\n",
    "    \n",
    "    dilated_03 = Conv2D(10, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(concatenate_02)\n",
    "    dilated_03 = Activation(activation_hidden)(dilated_03)\n",
    "    dilated_03 = Conv2D(10, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_03)       \n",
    "    dilated_03 = Activation(activation_hidden)(dilated_03)\n",
    "    \n",
    "    concatenate_03 = concatenate([conv03, dilated_03], axis=3)\n",
    "    \n",
    "    transpose_03 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(concatenate_03)       \n",
    "    transpose_03 = Activation(activation_hidden)(transpose_03)\n",
    "    transpose_03 = Conv2DTranspose(10, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_03)       \n",
    "    transpose_03 = Activation(activation_hidden)(transpose_03)\n",
    "         \n",
    "    concatenate_04 = concatenate([concatenate_02, transpose_03], axis=3)\n",
    "\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(concatenate_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(1, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    \n",
    "    concatenate_05 = concatenate([concatenate_01, transpose_04], axis=3)\n",
    "    #concatenate_05 = Conv2D(1, kernel_size=(1, 1),padding=\"same\")(concatenate_05)\n",
    "    output = concatenate_05\n",
    "    \n",
    "    max_count = output[:,:,:,:1]\n",
    "    prob      = output[:,:,:,1:2]\n",
    "    max_count = Flatten()(max_count)\n",
    "    prob      = Flatten()(prob)\n",
    "    prob      = Dense(8*8)(prob)\n",
    "    #prob      = Dropout(0.1)(prob)\n",
    "    \n",
    "    max_count      = Dense(10*10)(max_count)\n",
    "    #max_count      = Dropout(0.1)(max_count)\n",
    "    \n",
    "    prob      = Dense(256*256,activation=\"sigmoid\")(prob)\n",
    "    max_count = Dense(256*256,activation=\"sigmoid\")(max_count)\n",
    "    prob      = tf.keras.layers.Reshape((256,256,1))(prob)\n",
    "    max_count = tf.keras.layers.Reshape((256,256,1))(max_count)\n",
    "    #max_count = Activation(activation_hidden)(max_count)\n",
    "    input_dist= tf.concat([tf.math.softplus(max_count),prob],axis=-1)\n",
    "    \n",
    "    #print(max_count)\n",
    "    \n",
    "    output_dist = tfp.layers.DistributionLambda(name=\"DistributionLayer\",\n",
    "        make_distribution_fn=lambda t: tfd.Independent(tfp.distributions.NegativeBinomial(\n",
    "            total_count=t[...,0:1] , probs=t[..., 1:2] ),\n",
    "                                                      reinterpreted_batch_ndims=-1 ))\n",
    "    output = output_dist(input_dist)\n",
    "    #output = output_dist(prob)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def Indentpendent_Poisson_Unet_BigBrainTime(input_shape,\n",
    "                           activation_hidden=\"relu\",\n",
    "                           activation_output=\"relu\"):\n",
    "    \n",
    "    inputs = Input(shape=input_shape) \n",
    "\n",
    "    conv00 = Conv2D(5, kernel_size=(5, 5), dilation_rate=3,padding=\"same\")(inputs)\n",
    "    conv00 = Activation(activation_hidden)(conv00)\n",
    "    conv00 = Conv2D(10, kernel_size=(5, 5), strides=4,padding=\"same\")(conv00)\n",
    "    conv00 = Activation(activation_hidden)(conv00)\n",
    "    \n",
    "    \n",
    "    conv01 = Conv2D(5, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(conv00)       \n",
    "    conv01 = Activation(activation_hidden)(conv01)\n",
    "    \n",
    "    \n",
    "    dilated_01 = Conv2D(5, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(conv00)\n",
    "    dilated_01 = Activation(activation_hidden)(dilated_01)\n",
    "    dilated_01 = Conv2D(5, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_01)       \n",
    "    dilated_01 = Activation(activation_hidden)(dilated_01)\n",
    "    \n",
    "    concatenate_01 = concatenate([conv01, dilated_01], axis=3)\n",
    "    \n",
    "    conv02 = Conv2D(10, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(concatenate_01)       \n",
    "    conv02 = Activation(activation_hidden)(conv02)\n",
    "    \n",
    "    dilated_02 = Conv2D(10, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(concatenate_01)\n",
    "    dilated_02 = Activation(activation_hidden)(dilated_02)\n",
    "    dilated_02 = Conv2D(10, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_02)       \n",
    "    dilated_02 = Activation(activation_hidden)(dilated_02)\n",
    "    \n",
    "    concatenate_02 = concatenate([conv02, dilated_02], axis=3)\n",
    "    \n",
    "    conv03 = Conv2D(10, kernel_size=(5, 5), strides=(2,2),padding=\"same\")(concatenate_02)       \n",
    "    conv03 = Activation(activation_hidden)(conv03)\n",
    "    \n",
    "    dilated_03 = Conv2D(10, kernel_size=(5, 5),padding=\"same\",dilation_rate=2)(concatenate_02)\n",
    "    dilated_03 = Activation(activation_hidden)(dilated_03)\n",
    "    dilated_03 = Conv2D(10, kernel_size=(3, 3), strides=(2,2),padding=\"same\")(dilated_03)       \n",
    "    dilated_03 = Activation(activation_hidden)(dilated_03)\n",
    "    \n",
    "    concatenate_03 = concatenate([conv03, dilated_03], axis=3)\n",
    "    \n",
    "    transpose_03 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(concatenate_03)       \n",
    "    transpose_03 = Activation(activation_hidden)(transpose_03)\n",
    "    transpose_03 = Conv2DTranspose(10, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_03)       \n",
    "    transpose_03 = Activation(activation_hidden)(transpose_03)\n",
    "         \n",
    "    concatenate_04 = concatenate([concatenate_02, transpose_03], axis=3)\n",
    "\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(concatenate_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(5, 5),dilation_rate=(3,3))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(10, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    transpose_04 = Conv2DTranspose(1, kernel_size=(3, 3),dilation_rate=(2,2))(transpose_04)       \n",
    "    transpose_04 = Activation(activation_hidden)(transpose_04)\n",
    "    \n",
    "    concatenate_05 = concatenate([concatenate_01, transpose_04], axis=3)\n",
    "    concatenate_05 = Conv2D(1, kernel_size=(1, 1),padding=\"same\")(concatenate_05)\n",
    "    output = concatenate_05\n",
    "    \n",
    "    rate = output[:,:,:,:1]\n",
    "\n",
    "    rate = Flatten()(rate)\n",
    "\n",
    "    \n",
    "    rate   = Dense(8*8)(rate)\n",
    "    #max_count      = Dropout(0.1)(max_count)\n",
    "    \n",
    "\n",
    "    rate   = Dense(256*256,activation=\"sigmoid\")(rate)\n",
    "    rate   = tf.keras.layers.Reshape((256,256,1))(rate)\n",
    "\n",
    "    #rate = Activation(activation_hidden)(rate)\n",
    "    #input_dist= tf.concat([max_count,prob],axis=-1)\n",
    "    \n",
    "    #print(max_count)\n",
    "    output_dist = tfp.layers.DistributionLambda(name=\"DistributionLayer\",\n",
    "        make_distribution_fn=lambda t: tfd.Independent(tfp.distributions.Poisson(\n",
    "            rate=tf.math.softplus(t[...])),\n",
    "                                                      reinterpreted_batch_ndims=-1 ))\n",
    "    output = output_dist(rate)\n",
    "    #output = output_dist(prob)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded file]  ./model_data/negative_Binomial_Unet_BigBrainTime_function/negative_Binomial_Unet_BigBrainTime_function512x512x5.h5\n",
      "15\n",
      "[Loaded file]  ./model_data/negative_Binomial_Unet_BigBrainTime_function/negative_Binomial_Unet_BigBrainTime_function512x512x5history.json\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 512, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 512, 5)  630         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 512, 512, 5)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 10) 1260        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 10) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 5)  1255        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 5)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 5)    1255        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 5)    230         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 5)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 5)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 64, 10)   0           activation_2[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 10)   2510        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 10)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 10)   2510        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 10)   910         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 10)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 10)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 20)   0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 10)   5010        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 10)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 10)   5010        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 10)   910         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 10)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 10)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 20)   0           activation_8[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 28, 28, 10)   5010        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 10)   0           conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 10)   910         activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 10)   0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 30)   0           concatenate_1[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 44, 44, 10)   7510        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 44, 44, 10)   0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 56, 56, 10)   2510        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 56, 56, 10)   0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 60, 60, 10)   910         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 60, 60, 10)   0           conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 1)    91          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 64, 1)    0           conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 11)   0           concatenate[0][0]                \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 64, 64, 1)]  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 64, 64, 1)]  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          409700      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 65536)        6619136     dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           262208      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 256, 256, 1)  0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 65536)        4259840     dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softplus (TensorFlo [(None, 256, 256, 1) 0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 256, 256, 1)  0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 256, 256, 2) 0           tf_op_layer_Softplus[0][0]       \n",
      "                                                                 reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "DistributionLayer (Distribution ((None, 256, 256, 1) 0           tf_op_layer_concat[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 11,589,315\n",
      "Trainable params: 11,589,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "(<tfp.distributions.Independent 'DistributionLayer_IndependentDistributionLayer_NegativeBinomial' batch_shape=[?, 256, 256, 1] event_shape=[] dtype=float32>, <tf.Tensor 'DistributionLayer/Identity:0' shape=(None, 256, 256, 1) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam( lr = 1e-4 )\n",
    "trainer = Trainer(negative_Binomial_Unet_BigBrainTime,\n",
    "                    lossfunction=NLL,\n",
    "                    pathToData=(train,test),\n",
    "                    batch_size = batch_size,\n",
    "                    optimizer=optimizer,\n",
    "                    dimension = dimension,\n",
    "                    channels = channels,\n",
    "                    metrics = [\"mse\",\"mae\"])\n",
    "layer_name=\"DistributionLayer\"\n",
    "print(trainer.model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Dokumente/Teamprojekt/DeepRain2/Networks/Utils/transform.py:120: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return img[self.slices]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2672 - mse: 195.4861 - mae: 1.1535\n",
      "Epoch 00016: val_loss improved from inf to 0.36341, saving model to ./model_data/negative_Binomial_Unet_BigBrainTime_function/model-016-0.267209-0.363407.h5\n",
      "2479/2479 [==============================] - 1430s 577ms/step - loss: 0.2672 - mse: 195.4861 - mae: 1.1535 - val_loss: 0.3634 - val_mse: 226.1078 - val_mae: 1.4646\n",
      "Epoch 17/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2646 - mse: 156.6113 - mae: 1.1072\n",
      "Epoch 00017: val_loss improved from 0.36341 to 0.36330, saving model to ./model_data/negative_Binomial_Unet_BigBrainTime_function/model-017-0.264642-0.363299.h5\n",
      "2479/2479 [==============================] - 1422s 574ms/step - loss: 0.2646 - mse: 156.6113 - mae: 1.1072 - val_loss: 0.3633 - val_mse: 89.5594 - val_mae: 1.0843\n",
      "Epoch 18/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2642 - mse: 157.7127 - mae: 1.1024\n",
      "Epoch 00018: val_loss improved from 0.36330 to 0.36289, saving model to ./model_data/negative_Binomial_Unet_BigBrainTime_function/model-018-0.264174-0.362885.h5\n",
      "2479/2479 [==============================] - 1436s 579ms/step - loss: 0.2642 - mse: 157.7127 - mae: 1.1024 - val_loss: 0.3629 - val_mse: 94.9243 - val_mae: 1.0939\n",
      "Epoch 19/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2640 - mse: 155.0315 - mae: 1.1006\n",
      "Epoch 00019: val_loss improved from 0.36289 to 0.36261, saving model to ./model_data/negative_Binomial_Unet_BigBrainTime_function/model-019-0.264046-0.362607.h5\n",
      "2479/2479 [==============================] - 1439s 580ms/step - loss: 0.2640 - mse: 155.0315 - mae: 1.1006 - val_loss: 0.3626 - val_mse: 110.6416 - val_mae: 1.1588\n",
      "Epoch 20/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2638 - mse: 155.2975 - mae: 1.0990\n",
      "Epoch 00020: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1431s 577ms/step - loss: 0.2638 - mse: 155.2975 - mae: 1.0990 - val_loss: 0.3645 - val_mse: 79.2970 - val_mae: 1.0384\n",
      "Epoch 21/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2637 - mse: 155.8508 - mae: 1.0991\n",
      "Epoch 00021: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1434s 579ms/step - loss: 0.2637 - mse: 155.8508 - mae: 1.0991 - val_loss: 0.3634 - val_mse: 89.1800 - val_mae: 1.0814\n",
      "Epoch 22/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2636 - mse: 154.5346 - mae: 1.0977\n",
      "Epoch 00022: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1472s 594ms/step - loss: 0.2636 - mse: 154.5346 - mae: 1.0977 - val_loss: 0.3633 - val_mse: 90.0327 - val_mae: 1.0794\n",
      "Epoch 23/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2635 - mse: 157.2501 - mae: 1.0978\n",
      "Epoch 00023: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1452s 586ms/step - loss: 0.2635 - mse: 157.2501 - mae: 1.0978 - val_loss: 0.3629 - val_mse: 110.8316 - val_mae: 1.1402\n",
      "Epoch 24/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2633 - mse: 155.2042 - mae: 1.0963\n",
      "Epoch 00024: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1457s 588ms/step - loss: 0.2633 - mse: 155.2042 - mae: 1.0963 - val_loss: 0.3640 - val_mse: 91.7864 - val_mae: 1.0771\n",
      "Epoch 25/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2632 - mse: 155.0172 - mae: 1.0962\n",
      "Epoch 00025: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1432s 578ms/step - loss: 0.2632 - mse: 155.0172 - mae: 1.0962 - val_loss: 0.3631 - val_mse: 113.2254 - val_mae: 1.1452\n",
      "Epoch 26/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2630 - mse: 158.2665 - mae: 1.0964\n",
      "Epoch 00026: val_loss did not improve from 0.36261\n",
      "2479/2479 [==============================] - 1441s 581ms/step - loss: 0.2630 - mse: 158.2665 - mae: 1.0964 - val_loss: 0.3636 - val_mse: 93.1773 - val_mae: 1.0823\n",
      "Epoch 27/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2630 - mse: 156.5111 - mae: 1.0954\n",
      "Epoch 00027: val_loss improved from 0.36261 to 0.36255, saving model to ./model_data/negative_Binomial_Unet_BigBrainTime_function/model-027-0.263005-0.362550.h5\n",
      "2479/2479 [==============================] - 1444s 583ms/step - loss: 0.2630 - mse: 156.5111 - mae: 1.0954 - val_loss: 0.3626 - val_mse: 114.9289 - val_mae: 1.1552\n",
      "Epoch 28/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2627 - mse: 154.8481 - mae: 1.0934\n",
      "Epoch 00028: val_loss did not improve from 0.36255\n",
      "2479/2479 [==============================] - 1447s 584ms/step - loss: 0.2627 - mse: 154.8481 - mae: 1.0934 - val_loss: 0.3635 - val_mse: 117.2650 - val_mae: 1.1567\n",
      "Epoch 29/30\n",
      "2479/2479 [==============================] - ETA: 0s - loss: 0.2628 - mse: 160.5617 - mae: 1.0951\n",
      "Epoch 00029: val_loss did not improve from 0.36255\n",
      "2479/2479 [==============================] - 1454s 587ms/step - loss: 0.2628 - mse: 160.5617 - mae: 1.0951 - val_loss: 0.3638 - val_mse: 103.8827 - val_mae: 1.1120\n",
      "Epoch 30/30\n",
      "1251/2479 [==============>...............] - ETA: 8:54 - loss: 0.2634 - mse: 150.1262 - mae: 1.0893"
     ]
    }
   ],
   "source": [
    "trainer.fit(15)\n",
    "#m = trainer.model\n",
    "#print(m.layers[1].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainer.model\n",
    "w = m.layers[1].get_weights()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "history = trainer.history\n",
    "def plotHistory():\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['mse'])\n",
    "    plt.plot(history['val_mse'])\n",
    "    plt.title('Model MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "plotHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "pred = None\n",
    "label = None\n",
    "pred_ = 20\n",
    "test.on_epoch_end()\n",
    "mean = []\n",
    "stdev = []\n",
    "label = []\n",
    "prediction = []\n",
    "for x,y in test:\n",
    "    for i in range(batch_size):\n",
    "        if y[i,:,:,:].max() >= 0:\n",
    "            pred = model(np.array([x[i,:,:,:]]))\n",
    "            prediction.append(pred)\n",
    "            mean.append(pred.mean())\n",
    "            stdev.append(pred.stddev())\n",
    "            label.append(y[i,:,:,:])\n",
    "    if len(label) >= pred_:\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "fig, axes = plt.subplots(20, 3, figsize=(16,8*10),dpi=64)\n",
    "#fig.set_title([\"mean\",\"stdev\",\"label\"])\n",
    "for batch,img in enumerate(mean):\n",
    "    if batch == 20:\n",
    "        break\n",
    "    axes[batch,0].imshow(stdev[batch][0,:,:,0],cmap=\"gray\")\n",
    "    axes[batch,1].imshow(img[0,:,:,0],cmap=\"gray\")\n",
    "    axes[batch,2].imshow(label[batch][:,:,0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_Dist_To_Rain(prediction,label):\n",
    "    \n",
    "    x,y,t      = prediction[0][0,:,:,:].shape\n",
    "    rain_true  = np.zeros((x,y,t))\n",
    "    rain_false = rain_true.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ones = np.ones((x,y))\n",
    "    zeros = np.zeros((x,y,t))\n",
    "    value_map = np.array([ones * i for i in range(256)])\n",
    "    value_map = value_map.transpose(1,2,0)\n",
    "    for i,pred in enumerate(prediction):\n",
    "        ones = np.ones((x,y,t)) * 3/255\n",
    "        #print((np.array(pred.prob(ones))).max(axis=-1) )\n",
    "        probs = np.array(pred.prob(value_map))\n",
    "        print(label[i].max())\n",
    "        max_probs = probs.argmax(axis=-1)\n",
    "        #print(probs[0,1,1,:].argmax())\n",
    "\n",
    "        #print(probs.argmax(axis=-1))\n",
    "        #print(probs.max(axis=-1))\n",
    "        for j in range(256):\n",
    "            for k in range(256):\n",
    "                at = np.argmax(probs[0,k,j,:])\n",
    "                if at > 0:\n",
    "                #at = probs[0,k,j,:] .sum()\n",
    "                    print(\"AATT\",at)\n",
    "        #    print(j,\"min:\",probs[:,:,:,j].min(),\"max:\",probs[:,:,:,j].max())\n",
    "        #print(probs.shape)\n",
    "        #print(probs)\n",
    "        #print(pred.prob(0.0))\n",
    "        print(np.array(pred.prob(label[i].max())).max() )\n",
    "        print(np.array(pred.prob(label[i].max())).min() )\n",
    "        if label[i].max() == 0:\n",
    "            continue\n",
    "\n",
    "        print(pred)\n",
    "        s = (1-pred.cdf(zeros))\n",
    "        \n",
    "        #print(np.array(s).max(),np.array(probs.mean()).max(),label[i].max())\n",
    "        \n",
    "        plt.imshow(max_probs[0,:,:],cmap=\"gray\")\n",
    "        plt.show()\n",
    "        plt.imshow(s[0,:,:,0],cmap=\"gray\")\n",
    "        plt.show()\n",
    "        plt.imshow(label[i][:,:,0],cmap=\"gray\")\n",
    "        plt.show()\n",
    "        break\n",
    "    \n",
    "\n",
    "map_Dist_To_Rain(prediction,label)\n",
    "#print(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(255)\n",
    "print(label[0].max())\n",
    "idx = np.where(label[0] == label[0].max() )\n",
    "print(idx)\n",
    "pred = prediction[0]\n",
    "for i in range(255):\n",
    "    a = np.ones((256,256,1)) * i\n",
    "    #print(i,np.array(pred.prob(a)).max() )\n",
    "#print(np.array(pred.mean())[33,28])\n",
    "y = []\n",
    "x = []\n",
    "for i in range(256):\n",
    "    x.append(i)\n",
    "    #print(i,np.array(pred.prob(i))[0,33,28,:] )\n",
    "    y.append(np.array(pred.prob(i))[0,idx[0],idx[1],0][0])\n",
    "print(np.array(pred.mean())[0,idx[0],idx[1],0][0])\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "sns.set(rc={'figure.figsize':(200,8.27)})\n",
    "sns.barplot(x,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "propability = np.array(prediction[0].prob(label[0]))\n",
    "mean = np.array(prediction[0].mean())\n",
    "\n",
    "\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        print(propability[0,i,j,0],\"\\t\",mean[0,i,j,0],\"\\t\",label[0][i,j,0],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(propability.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
