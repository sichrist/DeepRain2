{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n"
     ]
    }
   ],
   "source": [
    "from Utils.loadset import getDataSet\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "import os\n",
    "from trainer import Trainer\n",
    "try:\n",
    "    from Utils.connection_cfg import *\n",
    "except Exception as e:\n",
    "    PSWD = None\n",
    "    USRN = None\n",
    "    \n",
    "from Utils.Data import dataWrapper\n",
    "from Utils.transform import ToCategorical, cutOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLayer(inp,nbrLayer,channel,activation=\"relu\"):\n",
    "    assert nbrLayer > 0, \"In Function convLayer nbrLayer > 0 ?\"\n",
    "    layer = Conv2D(channel, kernel_size=(3, 3), padding=\"same\") (inp)\n",
    "    layer = Activation(activation)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    \n",
    "    for i in range(1,nbrLayer):\n",
    "        layer = Conv2D(channel, kernel_size=(3, 3), padding=\"same\")  (layer)\n",
    "        layer = Activation(activation)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "    return layer\n",
    "\n",
    "#def FullUnetLoop(input_shape,down_channels=[64,128,256,512,1024],downLayer=2,activation=\"relu\"):\n",
    "\n",
    "def FullUnetLoop(input_shape,down_channels=[64,128,256,512],downLayer=2,activation=\"selu\"):\n",
    "\n",
    "    def zeroInflatedPoisson(output):\n",
    "        rate = tf.math.softplus(output[:,:,:,0:1]) #A \n",
    "        s = tf.math.sigmoid(output[:,:,:,1:2])\n",
    "        components = [tfd.Deterministic(loc=tf.zeros_like(rate)), #E\n",
    "         tfd.Poisson(rate=rate) #F \n",
    "         ]\n",
    "        mixture = tfd.Mixture(\n",
    "              cat=tfd.Categorical(probs=tf.stack([1-s, s],axis=-1)),#D\n",
    "              components=components)\n",
    "        return mixture\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    layer = Conv2D(down_channels[0], kernel_size=(3, 3), padding=\"same\") (inputs)\n",
    "    layer = Activation(activation)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    \n",
    "    layer = Conv2D(down_channels[0], kernel_size=(3, 3), padding=\"same\") (layer)\n",
    "    layer = Activation(activation)(layer)\n",
    "    firstLayer = BatchNormalization()(layer)\n",
    "    \n",
    "    pool  = MaxPooling2D((2, 2), strides=(2, 2))(firstLayer)\n",
    "    \n",
    "    layerArray = []\n",
    "    \n",
    "    for channel in down_channels[1:]:\n",
    "        \n",
    "        layer = convLayer(pool,downLayer,channel)\n",
    "       \n",
    "        if channel != down_channels[-1]:\n",
    "            layerArray.append(layer)\n",
    "            pool  = MaxPooling2D((2, 2), strides=(2, 2))(layer)\n",
    "            \n",
    "    for i,channel in enumerate(reversed(down_channels[:-1])):\n",
    "        \n",
    "        layer = Conv2DTranspose(channel,(3, 3),strides=(2,2),padding=\"same\")(layer)\n",
    "        layer = Activation(activation)(layer)\n",
    "        layer = BatchNormalization() (layer)\n",
    "        \n",
    "        if len(layerArray) >= (i+1):\n",
    "            layer = concatenate([layerArray[-(i+1)], layer], axis=3)\n",
    "        else:\n",
    "            layer = concatenate([firstLayer, layer], axis=3)\n",
    "        \n",
    "        layer = convLayer(layer,downLayer,channel)\n",
    "        \n",
    "    output = Conv2D(1, kernel_size=(1, 1), padding=\"same\",activation=tf.math.softplus) (layer)\n",
    "    #output = Conv2D(1, kernel_size=(1, 1), padding=\"same\",activation=None) (output)\n",
    "    #layer = Activation(\"softmax\")(output)\n",
    "    #output = Activation(\"relu\")(output)\n",
    "    #output = Activation(tf.math.softplus)(output)\n",
    "\n",
    "    #output = tfp.layers.DistributionLambda(zeroInflatedPoisson)(output)\n",
    "    \n",
    "    output = Flatten()(output)\n",
    "    output = Dense(32*32)(output)\n",
    "    output = tfp.layers.IndependentPoisson( (32,32,1) )(output)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mFound Year \u001b[0m:  2017 => won't download this year again... please check for consistency\n",
      "\u001b[32mFinished Loading Dataset\n",
      " \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dimension = (32,32)\n",
    "batch_size = 100\n",
    "channels = 5\n",
    "optimizer = Adam( lr = 1e-3 )\n",
    "slices =[272,304,272,304]\n",
    "cutOutFrame = cutOut(slices)\n",
    "\n",
    "categorical_list = [0,1,5,10,15,30,60,120]\n",
    "categorical = ToCategorical(categorical_list)\n",
    "\n",
    "PRETRAINING_TRANSFORMATIONS = [cutOutFrame]\n",
    "TRANSFORMATIONS = [categorical]\n",
    "TRANSFORMATIONS = None\n",
    "\n",
    "def NLL(y_true, y_hat):\n",
    "    return -y_hat.log_prob(y_true)\n",
    "\n",
    "\n",
    "def provideData(flatten=False,dimension=dimension,batch_size=60,transform=None,preTransformation=None):\n",
    "\n",
    "    getDataSet(DatasetFolder,year=[2017],username=USRN,pswd=PSWD)\n",
    "    train,test = dataWrapper(PathToData,\n",
    "                            dimension=dimension,\n",
    "                            channels=channels,\n",
    "                            batch_size=batch_size,\n",
    "                            overwritecsv=True,\n",
    "                            flatten=flatten,\n",
    "                            onlyUseYears=[2017],\n",
    "                            transform=transform,\n",
    "                            preTransformation=preTransformation)\n",
    "    \n",
    "    return train,test\n",
    "DatasetFolder = \"./Data/RAW\"\n",
    "PathToData = os.path.join(DatasetFolder,\"MonthPNGData\")\n",
    "\n",
    "train, test = provideData(dimension=dimension,\n",
    "                          batch_size=batch_size,\n",
    "                          transform=TRANSFORMATIONS,\n",
    "                          preTransformation=PRETRAINING_TRANSFORMATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load file failed]  ./model_data/FullUnetLoop_function/FullUnetLoop_function64x64x5.h5\n",
      "[Load file failed]  ./model_data/FullUnetLoop_function/FullUnetLoop_function64x64x5history.json\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 5)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 64)   2944        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 64, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 64)   256         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 64)   256         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 92)   53084       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 92)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 92)   368         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 92)   76268       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 92)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 92)   368         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 92)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 128)  106112      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 128)  0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 128)  147584      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 128)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 128)  512         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 8, 256)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 256)    1024        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 8, 8, 256)    590080      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 8, 8, 256)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 8, 8, 256)    1024        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 512)    1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 4, 4, 512)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4, 4, 512)    2048        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 512)    2359808     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 4, 4, 512)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 4, 4, 512)    2048        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 8, 8, 256)    1179904     batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 256)    0           conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 256)    1024        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8, 8, 512)    0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 256)    1179904     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 256)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 256)    1024        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 256)    590080      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 256)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 256)    1024        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  295040      batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 256)  0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 128)  295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 128)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147584      batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 128)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 92)   106076      batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 92)   0           conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 92)   368         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 184)  0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 92)   152444      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 92)   0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 92)   368         activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 92)   76268       batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 32, 32, 92)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 92)   368         activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 64, 64, 64)   53056       batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 64, 64)   0           conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, 64, 64)   256         activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64, 64, 128)  0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 64, 64)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, 64, 64)   256         activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 64)   36928       batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 64, 64)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 64, 64, 64)   256         activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 64, 64, 1)    65          batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "independent_poisson (Independen ((None, 64, 64, 1),  0           flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,049,213\n",
      "Trainable params: 9,041,765\n",
      "Non-trainable params: 7,448\n",
      "__________________________________________________________________________________________________\n",
      "len train,val 94 94\n"
     ]
    }
   ],
   "source": [
    "t = Trainer(FullUnetLoop,\n",
    "                    NLL,\n",
    "                    (train,test),\n",
    "                    batch_size = batch_size,\n",
    "                    optimizer=optimizer,\n",
    "                    dimension = dimension,\n",
    "                    channels = channels,\n",
    "                    metrics = [\"mse\",\"mae\"])\n",
    "\n",
    "print(\"len train,val\",len(train),len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 94 steps, validate for 94 steps\n",
      "Epoch 1/200\n",
      "94/94 [==============================] - 224s 2s/step - loss: 88067217.1432 - mse: 1714494304681984.0000 - mae: 35225.0312 - val_loss: 9592.7940 - val_mse: 9.9638 - val_mae: 2.3010\n",
      "Epoch 2/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 53720099.4612 - mse: 865403563671552.0000 - mae: 9520.9404 - val_loss: 10706.7035 - val_mse: 12.9424 - val_mae: 2.4471\n",
      "Epoch 3/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 562139.9123 - mse: 4498184192.0000 - mae: 152.6097 - val_loss: 9976.0851 - val_mse: 10.7011 - val_mae: 2.3740\n",
      "Epoch 4/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 700693.9797 - mse: 21701349376.0000 - mae: 240.5480 - val_loss: 10295.0542 - val_mse: 11.7739 - val_mae: 2.4212\n",
      "Epoch 5/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 966068.3857 - mse: 10050999296.0000 - mae: 127.8218 - val_loss: 10443.7768 - val_mse: 13.6491 - val_mae: 2.4358\n",
      "Epoch 6/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 123239.6203 - mse: 78817440.0000 - mae: 38.6123 - val_loss: 11238.5729 - val_mse: 28.3937 - val_mae: 2.5303\n",
      "Epoch 7/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 396694.0527 - mse: 1946540288.0000 - mae: 122.3782 - val_loss: 11641.9651 - val_mse: 268.5230 - val_mae: 2.5802\n",
      "Epoch 8/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 2852206.1773 - mse: 120028520448.0000 - mae: 453.8305 - val_loss: 12142.3298 - val_mse: 13310.1514 - val_mae: 3.0166\n",
      "Epoch 9/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 190875.7446 - mse: 382342048.0000 - mae: 63.3799 - val_loss: 35855.6346 - val_mse: 2126016.0000 - val_mae: 9.4937\n",
      "Epoch 10/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 47919050.0080 - mse: 122796940197888.0000 - mae: 3093.2368 - val_loss: 36135.8081 - val_mse: 3111436.5000 - val_mae: 8.5732\n",
      "Epoch 11/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 122492.9668 - mse: 45034056.0000 - mae: 29.5862 - val_loss: 25942.6244 - val_mse: 1065193.2500 - val_mae: 5.9993\n",
      "Epoch 12/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 187450.5161 - mse: 175951008.0000 - mae: 36.7642 - val_loss: 15171.2047 - val_mse: 165072.1562 - val_mae: 3.8616\n",
      "Epoch 13/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 520938.8899 - mse: 9772733440.0000 - mae: 98.6422 - val_loss: 62117.8115 - val_mse: 12112193.0000 - val_mae: 15.6313\n",
      "Epoch 14/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 679073.7864 - mse: 4461020160.0000 - mae: 211.9156 - val_loss: 15734.3973 - val_mse: 393139.7188 - val_mae: 4.3195\n",
      "Epoch 15/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 12463788.3413 - mse: 52971660378112.0000 - mae: 2228.6689 - val_loss: 52618.4409 - val_mse: 6992418.5000 - val_mae: 14.2789\n",
      "Epoch 16/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 331392.0524 - mse: 10579984384.0000 - mae: 87.4366 - val_loss: 240329.8419 - val_mse: 1085984256.0000 - val_mae: 82.3474\n",
      "Epoch 17/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 33663.6590 - mse: 2535914.5000 - mae: 10.2441 - val_loss: 13131.1687 - val_mse: 21274.7129 - val_mae: 3.1699\n",
      "Epoch 18/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 181366.2921 - mse: 465476096.0000 - mae: 25.3020 - val_loss: 18834.4900 - val_mse: 88343.8906 - val_mae: 4.1085\n",
      "Epoch 19/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 134018.8459 - mse: 108328032.0000 - mae: 37.5416 - val_loss: 28600.6351 - val_mse: 920084.8750 - val_mae: 6.2738\n",
      "Epoch 20/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 60354.6334 - mse: 5398478.5000 - mae: 13.7172 - val_loss: 29413.0743 - val_mse: 1754685.2500 - val_mae: 6.5628\n",
      "Epoch 21/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 4568809.7838 - mse: 1237265154048.0000 - mae: 612.0734 - val_loss: 38152.5621 - val_mse: 14348406.0000 - val_mae: 12.2798\n",
      "Epoch 22/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 51726.8381 - mse: 17539380.0000 - mae: 13.3384 - val_loss: 51410.5332 - val_mse: 17636712.0000 - val_mae: 15.4202\n",
      "Epoch 23/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 990264.3450 - mse: 40053583872.0000 - mae: 277.2837 - val_loss: 111464.1435 - val_mse: 234160128.0000 - val_mae: 33.6646\n",
      "Epoch 24/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 385506.7046 - mse: 2825703936.0000 - mae: 123.6959 - val_loss: 13956.1380 - val_mse: 65872.3047 - val_mae: 3.3895\n",
      "Epoch 25/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 83019.7466 - mse: 114797200.0000 - mae: 21.9442 - val_loss: 24527.6001 - val_mse: 8045164.0000 - val_mae: 7.9688\n",
      "Epoch 26/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 1591989.1288 - mse: 48836562944.0000 - mae: 341.4073 - val_loss: 46629.7801 - val_mse: 16925484.0000 - val_mae: 10.9082\n",
      "Epoch 27/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 54405.2157 - mse: 3836995.5000 - mae: 12.8152 - val_loss: 38268.4680 - val_mse: 5903054.5000 - val_mae: 9.9328\n",
      "Epoch 28/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 26652.4275 - mse: 957254.8125 - mae: 7.8833 - val_loss: 17552.6346 - val_mse: 527831.3750 - val_mae: 4.6950\n",
      "Epoch 29/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 45646.5692 - mse: 4520732.5000 - mae: 12.5999 - val_loss: 20951.7980 - val_mse: 1577574.6250 - val_mae: 5.8738\n",
      "Epoch 30/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 34017.6757 - mse: 1674378.0000 - mae: 9.8079 - val_loss: 20221.9657 - val_mse: 408852.6562 - val_mae: 4.7335\n",
      "Epoch 31/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 113349.1092 - mse: 66953008.0000 - mae: 25.5823 - val_loss: 24757.7974 - val_mse: 5239158.5000 - val_mae: 7.0620\n",
      "Epoch 32/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 553289.7479 - mse: 27468828672.0000 - mae: 154.4049 - val_loss: 54718.7263 - val_mse: 141961136.0000 - val_mae: 14.6433\n",
      "Epoch 33/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 40011.3908 - mse: 12486390.0000 - mae: 12.5160 - val_loss: 27382.4641 - val_mse: 35811864.0000 - val_mae: 7.5033\n",
      "Epoch 34/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 27372.1394 - mse: 1365439.8750 - mae: 8.4038 - val_loss: 17618.3463 - val_mse: 404994.1875 - val_mae: 4.4401\n",
      "Epoch 35/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 107580.2090 - mse: 34710236.0000 - mae: 24.4520 - val_loss: 15385.8603 - val_mse: 495109.3438 - val_mae: 4.2719\n",
      "Epoch 36/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 1257364.3188 - mse: 392419704832.0000 - mae: 416.8770 - val_loss: 77399.3177 - val_mse: 14030396.0000 - val_mae: 18.8353\n",
      "Epoch 37/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 135121.2708 - mse: 122486664.0000 - mae: 32.6082 - val_loss: 157840.0268 - val_mse: 809843072.0000 - val_mae: 43.2862\n",
      "Epoch 38/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 78062.3200 - mse: 61410288.0000 - mae: 23.1615 - val_loss: 342703.6738 - val_mse: 4699309568.0000 - val_mae: 137.8634\n",
      "Epoch 39/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 89450.8390 - mse: 58264244.0000 - mae: 28.8851 - val_loss: 307807.5100 - val_mse: 20633008128.0000 - val_mae: 136.5395\n",
      "Epoch 40/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 131393.5156 - mse: 93068696.0000 - mae: 35.8172 - val_loss: 41341.0509 - val_mse: 19718554.0000 - val_mae: 15.1073\n",
      "Epoch 41/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 53661.9503 - mse: 20692640.0000 - mae: 14.8997 - val_loss: 21222.4841 - val_mse: 1962046.3750 - val_mae: 6.4291\n",
      "Epoch 42/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 23787.7171 - mse: 548976.9375 - mae: 6.7977 - val_loss: 15063.0978 - val_mse: 85260.1172 - val_mae: 3.3905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 43778.5316 - mse: 2747053.5000 - mae: 10.8515 - val_loss: 28335.7242 - val_mse: 24624374.0000 - val_mae: 9.6159\n",
      "Epoch 44/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 32364.4172 - mse: 1613082.8750 - mae: 9.1487 - val_loss: 35403.0834 - val_mse: 10646608.0000 - val_mae: 10.8244\n",
      "Epoch 45/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 54840.1832 - mse: 14826545.0000 - mae: 14.1441 - val_loss: 18818.1248 - val_mse: 452019.4375 - val_mae: 4.8051\n",
      "Epoch 46/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 56398.7168 - mse: 6014562.5000 - mae: 12.2075 - val_loss: 27628.5061 - val_mse: 1667788.8750 - val_mae: 6.4560\n",
      "Epoch 47/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 53442.5463 - mse: 2728456.7500 - mae: 10.4353 - val_loss: 30172.8493 - val_mse: 3649836.0000 - val_mae: 7.0544\n",
      "Epoch 48/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 1275027.4815 - mse: 25178687488.0000 - mae: 129.5359 - val_loss: 78808.8593 - val_mse: 110580232.0000 - val_mae: 23.7918\n",
      "Epoch 49/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 671786.1008 - mse: 22434678784.0000 - mae: 176.1969 - val_loss: 62561.8439 - val_mse: 33390196.0000 - val_mae: 18.2203\n",
      "Epoch 50/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 107674.5050 - mse: 84300752.0000 - mae: 19.7593 - val_loss: 1369292.7139 - val_mse: 12838112256.0000 - val_mae: 252.4541\n",
      "Epoch 51/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 30390.1482 - mse: 1767100.5000 - mae: 8.1225 - val_loss: 27248.0373 - val_mse: 19305802.0000 - val_mae: 8.9136\n",
      "Epoch 52/200\n",
      "94/94 [==============================] - 11s 120ms/step - loss: 57201.2826 - mse: 20516586.0000 - mae: 15.0368 - val_loss: 15666.5841 - val_mse: 185127.5938 - val_mae: 3.8077\n",
      "Epoch 53/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 24371.4897 - mse: 504562.6562 - mae: 6.4942 - val_loss: 16816.7424 - val_mse: 524931.8125 - val_mae: 4.7129\n",
      "Epoch 54/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 54699.0496 - mse: 21141396.0000 - mae: 12.6199 - val_loss: 32093.7876 - val_mse: 1266947.5000 - val_mae: 7.7082\n",
      "Epoch 55/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 59046.4125 - mse: 16744859.0000 - mae: 15.8496 - val_loss: 79793.0744 - val_mse: 6400013.0000 - val_mae: 12.4925\n",
      "Epoch 56/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 31105.2334 - mse: 1705264.3750 - mae: 7.7822 - val_loss: 19567.6340 - val_mse: 924785.4375 - val_mae: 5.4991\n",
      "Epoch 57/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 27372.9991 - mse: 654076.5000 - mae: 6.7070 - val_loss: 64097.7529 - val_mse: 15245976.0000 - val_mae: 12.1660\n",
      "Epoch 58/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 36843.1557 - mse: 3170864.0000 - mae: 9.5217 - val_loss: 36501.9212 - val_mse: 1710200.2500 - val_mae: 7.0923\n",
      "Epoch 59/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 381524.9176 - mse: 8851209216.0000 - mae: 28.6661 - val_loss: 17204.9107 - val_mse: 377054.4375 - val_mae: 4.4401\n",
      "Epoch 60/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 38056.0967 - mse: 3443137.7500 - mae: 9.9582 - val_loss: 16890.9767 - val_mse: 66871.7656 - val_mae: 3.7740\n",
      "Epoch 61/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 26807.4571 - mse: 454396.1250 - mae: 6.7306 - val_loss: 14964.2685 - val_mse: 577277.8125 - val_mae: 4.0431\n",
      "Epoch 62/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 3671190.3948 - mse: 3408563798016.0000 - mae: 522.8096 - val_loss: 195428.0686 - val_mse: 371767552.0000 - val_mae: 49.5083\n",
      "Epoch 63/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 24760.3992 - mse: 819779.6250 - mae: 6.9916 - val_loss: 74253.8165 - val_mse: 8293858.0000 - val_mae: 14.7365\n",
      "Epoch 64/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 60911.2909 - mse: 14026467.0000 - mae: 11.3578 - val_loss: 26986.3092 - val_mse: 2264447.7500 - val_mae: 7.4477\n",
      "Epoch 65/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 25289.8271 - mse: 919416.0625 - mae: 6.5429 - val_loss: 13892.4042 - val_mse: 82864.4609 - val_mae: 3.6422\n",
      "Epoch 66/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 40331.9263 - mse: 4421186.0000 - mae: 9.4436 - val_loss: 50386.1264 - val_mse: 15060898.0000 - val_mae: 13.4194\n",
      "Epoch 67/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 32407.0769 - mse: 1970465.2500 - mae: 9.1322 - val_loss: 24507.8303 - val_mse: 2488040.5000 - val_mae: 7.0616\n",
      "Epoch 68/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 24495.2255 - mse: 1055839.8750 - mae: 6.5258 - val_loss: 45276.1463 - val_mse: 3102446.7500 - val_mae: 8.1852\n",
      "Epoch 69/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 24537.9739 - mse: 669285.8750 - mae: 6.7369 - val_loss: 18727.1329 - val_mse: 221667.5000 - val_mae: 4.7442\n",
      "Epoch 70/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 48001.5367 - mse: 25097302.0000 - mae: 13.7883 - val_loss: 15842.7867 - val_mse: 92237.3984 - val_mae: 3.7524\n",
      "Epoch 71/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 42528.5878 - mse: 4503465.5000 - mae: 7.7986 - val_loss: 14778.9285 - val_mse: 119138.1328 - val_mae: 3.9024\n",
      "Epoch 72/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 38777.2689 - mse: 2570552.7500 - mae: 10.6206 - val_loss: 30837.8641 - val_mse: 1268143.3750 - val_mae: 6.7507\n",
      "Epoch 73/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 20978.7318 - mse: 189550.4531 - mae: 5.4861 - val_loss: 15612.4643 - val_mse: 134067.7969 - val_mae: 3.8974\n",
      "Epoch 74/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 92048.1937 - mse: 44299204.0000 - mae: 15.3361 - val_loss: 27751.1107 - val_mse: 839139.1875 - val_mae: 6.8324\n",
      "Epoch 75/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 63776.5395 - mse: 9287670.0000 - mae: 14.3091 - val_loss: 36382.2600 - val_mse: 4560292.5000 - val_mae: 10.0349\n",
      "Epoch 76/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 24270.2978 - mse: 481174.2188 - mae: 6.0066 - val_loss: 51820.5131 - val_mse: 5049058.5000 - val_mae: 13.3275\n",
      "Epoch 77/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 29443.9986 - mse: 6507002.0000 - mae: 8.7013 - val_loss: 30842.7382 - val_mse: 6440181.5000 - val_mae: 9.3139\n",
      "Epoch 78/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 32369.3056 - mse: 2640650.0000 - mae: 8.9581 - val_loss: 21464.7425 - val_mse: 2422474.2500 - val_mae: 6.2153\n",
      "Epoch 79/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 21176.4313 - mse: 225958.0781 - mae: 5.4960 - val_loss: 17584.0032 - val_mse: 266615.4062 - val_mae: 4.1100\n",
      "Epoch 80/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 26227.5040 - mse: 1059319.0000 - mae: 6.9788 - val_loss: 24380.5256 - val_mse: 122140.1641 - val_mae: 4.5806\n",
      "Epoch 81/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 23786.8434 - mse: 340900.5000 - mae: 6.0090 - val_loss: 12823.9617 - val_mse: 145589.3750 - val_mae: 3.3349\n",
      "Epoch 82/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 19454.7247 - mse: 103113.9062 - mae: 4.8703 - val_loss: 15486.7599 - val_mse: 338553.0000 - val_mae: 4.0352\n",
      "Epoch 83/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 63916.2706 - mse: 67619256.0000 - mae: 20.4515 - val_loss: 87204.3024 - val_mse: 84604760.0000 - val_mae: 27.1302\n",
      "Epoch 84/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 22565.0113 - mse: 181566.6875 - mae: 5.6330 - val_loss: 17029.0098 - val_mse: 229337.9219 - val_mae: 4.4040\n",
      "Epoch 85/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 23320.5406 - mse: 314336.4062 - mae: 5.9661 - val_loss: 18145.3106 - val_mse: 295963.5938 - val_mae: 4.4757\n",
      "Epoch 86/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 59231.7031 - mse: 10761225.0000 - mae: 16.3779 - val_loss: 69842.0503 - val_mse: 17059970.0000 - val_mae: 12.3290\n",
      "Epoch 87/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 38432.5573 - mse: 10814445.0000 - mae: 10.6500 - val_loss: 51532.8867 - val_mse: 14549722.0000 - val_mae: 11.8635\n",
      "Epoch 88/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 46913.4798 - mse: 4260134.5000 - mae: 11.1029 - val_loss: 38657.6613 - val_mse: 3407224.0000 - val_mae: 8.5425\n",
      "Epoch 89/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 22614.0528 - mse: 220465.5938 - mae: 5.7553 - val_loss: 22307.2237 - val_mse: 1106742.8750 - val_mae: 5.8448\n",
      "Epoch 90/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 47238.4213 - mse: 10565474.0000 - mae: 9.3000 - val_loss: 14796.6128 - val_mse: 86405.6016 - val_mae: 3.7543\n",
      "Epoch 91/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 18194.2052 - mse: 65259.4297 - mae: 4.3322 - val_loss: 12748.3565 - val_mse: 15130.1016 - val_mae: 3.0974\n",
      "Epoch 92/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17577.2821 - mse: 74495.9531 - mae: 4.4960 - val_loss: 14432.0462 - val_mse: 82491.3906 - val_mae: 3.5460\n",
      "Epoch 93/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 28484.9334 - mse: 1034070.8750 - mae: 7.1900 - val_loss: 19997.3668 - val_mse: 1413452.3750 - val_mae: 5.8125\n",
      "Epoch 94/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 288844.4786 - mse: 12395399168.0000 - mae: 61.6264 - val_loss: 85743.5956 - val_mse: 115536448.0000 - val_mae: 26.0313\n",
      "Epoch 95/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17732.1237 - mse: 40770.6250 - mae: 4.1735 - val_loss: 12842.0656 - val_mse: 51794.9297 - val_mae: 3.2994\n",
      "Epoch 96/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 19089.4228 - mse: 39522.1289 - mae: 4.3029 - val_loss: 13713.2397 - val_mse: 98515.3281 - val_mae: 3.5172\n",
      "Epoch 97/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 17515.7629 - mse: 55407.6406 - mae: 4.3689 - val_loss: 12672.6841 - val_mse: 7986.5610 - val_mae: 3.0761\n",
      "Epoch 98/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 20780.2664 - mse: 132896.5000 - mae: 5.0000 - val_loss: 13375.0583 - val_mse: 140230.2344 - val_mae: 3.3312\n",
      "Epoch 99/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 26642.3243 - mse: 1581291.2500 - mae: 4.9887 - val_loss: 17590.8284 - val_mse: 184630.2188 - val_mae: 4.2695\n",
      "Epoch 100/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 16547.9909 - mse: 26014.4336 - mae: 3.9888 - val_loss: 15474.1825 - val_mse: 95999.4375 - val_mae: 3.8905\n",
      "Epoch 101/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 61991.9111 - mse: 19270926.0000 - mae: 15.4151 - val_loss: 74391.6732 - val_mse: 6556467.0000 - val_mae: 10.4187\n",
      "Epoch 102/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 33153.2325 - mse: 1469290.1250 - mae: 8.5011 - val_loss: 569726.2723 - val_mse: 8106568704.0000 - val_mae: 186.5800\n",
      "Epoch 103/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 20232.4534 - mse: 102880.3047 - mae: 4.8938 - val_loss: 18664.6934 - val_mse: 596298.5000 - val_mae: 4.6306\n",
      "Epoch 104/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 19589.4540 - mse: 57901.3828 - mae: 4.6172 - val_loss: 16794.3584 - val_mse: 693406.9375 - val_mae: 4.4676\n",
      "Epoch 105/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 19197.7358 - mse: 118947.9453 - mae: 4.7232 - val_loss: 21374.7166 - val_mse: 1115470.7500 - val_mae: 6.0462\n",
      "Epoch 106/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 20915.0944 - mse: 205025.3125 - mae: 5.0834 - val_loss: 20141.6289 - val_mse: 227887.6094 - val_mae: 4.7599\n",
      "Epoch 107/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17563.2790 - mse: 44788.0078 - mae: 4.2973 - val_loss: 23326.8002 - val_mse: 1174788.0000 - val_mae: 5.6919\n",
      "Epoch 108/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 1279230.3340 - mse: 121775472640.0000 - mae: 117.3698 - val_loss: 25376.6362 - val_mse: 2230675.5000 - val_mae: 7.3669\n",
      "Epoch 109/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 28949.6359 - mse: 2535992.2500 - mae: 7.6905 - val_loss: 33834.0098 - val_mse: 5285607.5000 - val_mae: 10.0334\n",
      "Epoch 110/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 18934.6785 - mse: 125543.6172 - mae: 4.8520 - val_loss: 13808.0293 - val_mse: 379268.8750 - val_mae: 3.6731\n",
      "Epoch 111/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 21216.2902 - mse: 570693.3125 - mae: 4.9987 - val_loss: 17768.4292 - val_mse: 197704.8750 - val_mae: 4.4513\n",
      "Epoch 112/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 29429.4346 - mse: 2112345.7500 - mae: 6.1020 - val_loss: 26769.6216 - val_mse: 919355.3125 - val_mae: 6.8217\n",
      "Epoch 113/200\n",
      "94/94 [==============================] - 11s 121ms/step - loss: 20998.8844 - mse: 466261.2500 - mae: 4.6414 - val_loss: 24919.1897 - val_mse: 2009338.6250 - val_mae: 7.2334\n",
      "Epoch 114/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 60921.5993 - mse: 10859790.0000 - mae: 9.5664 - val_loss: 20503.8824 - val_mse: 1466916.2500 - val_mae: 5.9144\n",
      "Epoch 115/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 155449.5590 - mse: 248058496.0000 - mae: 29.7076 - val_loss: 41517.8853 - val_mse: 2000923.2500 - val_mae: 10.5805\n",
      "Epoch 116/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 17877.3711 - mse: 105021.5703 - mae: 4.2610 - val_loss: 14606.5020 - val_mse: 52707.4961 - val_mae: 3.6662\n",
      "Epoch 117/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15581.7836 - mse: 6739.0923 - mae: 3.6260 - val_loss: 12733.6793 - val_mse: 5824.5386 - val_mae: 3.0945\n",
      "Epoch 118/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 16508.9986 - mse: 20560.2930 - mae: 3.7853 - val_loss: 13445.1145 - val_mse: 36612.5352 - val_mae: 3.3433\n",
      "Epoch 119/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 16054.2838 - mse: 23409.6680 - mae: 3.9572 - val_loss: 12750.9284 - val_mse: 64705.5977 - val_mae: 3.2036\n",
      "Epoch 120/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 28743.8916 - mse: 3758447.0000 - mae: 7.6150 - val_loss: 18557.2446 - val_mse: 633738.1250 - val_mae: 4.8121\n",
      "Epoch 121/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 32446.5702 - mse: 6578010.5000 - mae: 7.1810 - val_loss: 25386.7378 - val_mse: 666362.8125 - val_mae: 6.0366\n",
      "Epoch 122/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 16960.0740 - mse: 75908.2969 - mae: 4.0603 - val_loss: 40383.3289 - val_mse: 12844168.0000 - val_mae: 10.9472\n",
      "Epoch 123/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 26015.9115 - mse: 1201859.8750 - mae: 5.4555 - val_loss: 81800.4303 - val_mse: 249116768.0000 - val_mae: 27.9675\n",
      "Epoch 124/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 20328.1248 - mse: 300359.2500 - mae: 4.2822 - val_loss: 44616.2674 - val_mse: 21492740.0000 - val_mae: 12.7451\n",
      "Epoch 125/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17004.5011 - mse: 16462.1895 - mae: 3.8403 - val_loss: 13555.5491 - val_mse: 84058.0547 - val_mae: 3.4805\n",
      "Epoch 126/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 26959.8468 - mse: 1084903.6250 - mae: 5.5309 - val_loss: 14302.1594 - val_mse: 19854.0723 - val_mae: 3.4396\n",
      "Epoch 127/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 18793.6488 - mse: 38483.9062 - mae: 4.1372 - val_loss: 15497.0530 - val_mse: 313757.1250 - val_mae: 4.0508\n",
      "Epoch 128/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 24137.3057 - mse: 2212287.0000 - mae: 5.1757 - val_loss: 25655.9922 - val_mse: 3653394.5000 - val_mae: 7.8132\n",
      "Epoch 129/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 16574.9812 - mse: 69034.0156 - mae: 3.9445 - val_loss: 16034.2501 - val_mse: 122818.4531 - val_mae: 3.7707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 18086.2217 - mse: 23305.5352 - mae: 3.9713 - val_loss: 37045.8910 - val_mse: 24373396.0000 - val_mae: 11.4779\n",
      "Epoch 131/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15984.8699 - mse: 19930.4082 - mae: 3.7839 - val_loss: 156411.4036 - val_mse: 174508304.0000 - val_mae: 32.1723\n",
      "Epoch 132/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 16666.9428 - mse: 141132.2344 - mae: 4.0406 - val_loss: 23284.2749 - val_mse: 4585970.5000 - val_mae: 6.9998\n",
      "Epoch 133/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 56448.7375 - mse: 40057692.0000 - mae: 7.9422 - val_loss: 29876.2779 - val_mse: 5450466.0000 - val_mae: 8.1185\n",
      "Epoch 134/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 19393.9253 - mse: 198124.4844 - mae: 4.1044 - val_loss: 29150.8608 - val_mse: 2095966.5000 - val_mae: 6.9246\n",
      "Epoch 135/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14750.0476 - mse: 3568.6050 - mae: 3.4030 - val_loss: 13096.6658 - val_mse: 8462.3066 - val_mae: 3.1837\n",
      "Epoch 136/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14337.4149 - mse: 2199.0725 - mae: 3.2819 - val_loss: 13743.4224 - val_mse: 20377.2930 - val_mae: 3.3345\n",
      "Epoch 137/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15910.1891 - mse: 6114.5469 - mae: 3.3995 - val_loss: 12987.2021 - val_mse: 15091.2734 - val_mae: 3.1202\n",
      "Epoch 138/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 19292.7108 - mse: 340866.6250 - mae: 4.3860 - val_loss: 28594.9809 - val_mse: 572374.5625 - val_mae: 6.1759\n",
      "Epoch 139/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 16684.9217 - mse: 29666.7637 - mae: 3.8956 - val_loss: 64337.7291 - val_mse: 25848480.0000 - val_mae: 15.4023\n",
      "Epoch 140/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14459.2091 - mse: 7401.9902 - mae: 3.4756 - val_loss: 15834.7485 - val_mse: 69131.0234 - val_mae: 3.7364\n",
      "Epoch 141/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 16556.4871 - mse: 26727.7266 - mae: 3.6189 - val_loss: 16185.1092 - val_mse: 37776.7344 - val_mae: 3.7475\n",
      "Epoch 142/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 18341.1030 - mse: 32656.2539 - mae: 3.8996 - val_loss: 16440.6591 - val_mse: 181870.4062 - val_mae: 4.0294\n",
      "Epoch 143/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14843.7965 - mse: 18714.2188 - mae: 3.6310 - val_loss: 14741.1852 - val_mse: 104444.6641 - val_mae: 3.9220\n",
      "Epoch 144/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17299.1440 - mse: 65552.4922 - mae: 4.3075 - val_loss: 22747.8803 - val_mse: 490319.2500 - val_mae: 5.8084\n",
      "Epoch 145/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15632.3610 - mse: 21510.1406 - mae: 3.4862 - val_loss: 22190.9839 - val_mse: 382638.9375 - val_mae: 4.8359\n",
      "Epoch 146/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 15157.1854 - mse: 12387.5762 - mae: 3.6488 - val_loss: 26534.3965 - val_mse: 256973.4062 - val_mae: 4.6487\n",
      "Epoch 147/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 16261.9696 - mse: 2645.6052 - mae: 3.3918 - val_loss: 12237.9065 - val_mse: 5637.8594 - val_mae: 3.0082\n",
      "Epoch 148/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17047.4309 - mse: 91168.8125 - mae: 4.2371 - val_loss: 11671.4346 - val_mse: 1884.1565 - val_mae: 2.8332\n",
      "Epoch 149/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 16834.3842 - mse: 14862.4014 - mae: 3.7662 - val_loss: 15974.6553 - val_mse: 140434.3594 - val_mae: 3.9381\n",
      "Epoch 150/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 14234.8418 - mse: 4124.2383 - mae: 3.3507 - val_loss: 13883.9858 - val_mse: 97252.4531 - val_mae: 3.5448\n",
      "Epoch 151/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 20878.5215 - mse: 91903.9219 - mae: 4.5147 - val_loss: 15748.5212 - val_mse: 314871.9375 - val_mae: 3.7485\n",
      "Epoch 152/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 20672.7507 - mse: 960484.9375 - mae: 5.2622 - val_loss: 25722.6668 - val_mse: 677434.3125 - val_mae: 6.3401\n",
      "Epoch 153/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 19060.0550 - mse: 226391.4219 - mae: 4.0746 - val_loss: 398413.6879 - val_mse: 2919838976.0000 - val_mae: 114.4208\n",
      "Epoch 154/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 19454.2296 - mse: 202874.6406 - mae: 3.8122 - val_loss: 29714.5960 - val_mse: 470591.7188 - val_mae: 6.1533\n",
      "Epoch 155/200\n",
      "94/94 [==============================] - 11s 122ms/step - loss: 16997.0796 - mse: 35484.4219 - mae: 3.6604 - val_loss: 13981.2767 - val_mse: 49425.7422 - val_mae: 3.5776\n",
      "Epoch 156/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 16142.1170 - mse: 26961.5059 - mae: 3.6712 - val_loss: 12747.9544 - val_mse: 1664.7229 - val_mae: 2.9913\n",
      "Epoch 157/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14463.2581 - mse: 2345.7417 - mae: 3.3059 - val_loss: 17654.9114 - val_mse: 123442.5000 - val_mae: 4.1845\n",
      "Epoch 158/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 14698.0176 - mse: 1713.8440 - mae: 3.2700 - val_loss: 13330.2199 - val_mse: 60633.6875 - val_mae: 3.3279\n",
      "Epoch 159/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 14719.9342 - mse: 3025.1826 - mae: 3.3036 - val_loss: 13744.0433 - val_mse: 59770.9922 - val_mae: 3.5897\n",
      "Epoch 160/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 15115.0114 - mse: 5400.5991 - mae: 3.4803 - val_loss: 13743.0026 - val_mse: 15698.2305 - val_mae: 3.1744\n",
      "Epoch 161/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 14121.7463 - mse: 1791.7201 - mae: 3.2104 - val_loss: 13676.5425 - val_mse: 11455.3682 - val_mae: 3.1466\n",
      "Epoch 162/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 25254.9403 - mse: 3082143.2500 - mae: 5.8331 - val_loss: 12794.7158 - val_mse: 13900.4512 - val_mae: 3.1220\n",
      "Epoch 163/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 18476.6152 - mse: 83409.1328 - mae: 3.7347 - val_loss: 14378.2992 - val_mse: 42330.6484 - val_mae: 3.4863\n",
      "Epoch 164/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 16229.0424 - mse: 15185.4834 - mae: 3.6361 - val_loss: 15324.0178 - val_mse: 106140.1406 - val_mae: 3.8416\n",
      "Epoch 165/200\n",
      "94/94 [==============================] - 12s 127ms/step - loss: 13890.2092 - mse: 3857.8008 - mae: 3.2513 - val_loss: 16976.6883 - val_mse: 62029.4219 - val_mae: 4.0977\n",
      "Epoch 166/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 14063.7685 - mse: 1481.1978 - mae: 3.1875 - val_loss: 12868.7897 - val_mse: 53486.9141 - val_mae: 3.1752\n",
      "Epoch 167/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14263.6454 - mse: 870.5218 - mae: 3.1349 - val_loss: 12849.2960 - val_mse: 1656.0660 - val_mae: 2.9237\n",
      "Epoch 168/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 14465.6878 - mse: 7446.3774 - mae: 3.4557 - val_loss: 13649.2575 - val_mse: 883.6804 - val_mae: 2.9054\n",
      "Epoch 169/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 18324.2196 - mse: 75429.4766 - mae: 3.7631 - val_loss: 13321.7958 - val_mse: 3673.0151 - val_mae: 3.1433\n",
      "Epoch 170/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 22520.8874 - mse: 462909.6562 - mae: 5.0932 - val_loss: 20434.7538 - val_mse: 217765.6250 - val_mae: 5.3159\n",
      "Epoch 171/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 13957.8271 - mse: 845.6058 - mae: 3.1662 - val_loss: 15927.6208 - val_mse: 417220.2188 - val_mae: 4.2233\n",
      "Epoch 172/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 13716.5416 - mse: 1770.2904 - mae: 3.2227 - val_loss: 12353.9255 - val_mse: 26032.4785 - val_mae: 3.0263\n",
      "Epoch 173/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14208.5296 - mse: 2572.2373 - mae: 3.2714 - val_loss: 13286.5097 - val_mse: 8562.4824 - val_mae: 3.2174\n",
      "Epoch 174/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 17437.9607 - mse: 550479.6250 - mae: 4.1013 - val_loss: 16705.6790 - val_mse: 621693.3125 - val_mae: 4.3348\n",
      "Epoch 175/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 13823.9974 - mse: 1168.1774 - mae: 3.1804 - val_loss: 13823.4474 - val_mse: 11972.1475 - val_mae: 3.1446\n",
      "Epoch 176/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 14780.3783 - mse: 33417.6328 - mae: 3.4787 - val_loss: 16478.5383 - val_mse: 97536.2969 - val_mae: 3.7422\n",
      "Epoch 177/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14219.9556 - mse: 6604.6201 - mae: 3.2765 - val_loss: 14824.7657 - val_mse: 41580.8047 - val_mae: 3.4675\n",
      "Epoch 178/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14019.8469 - mse: 6300.3379 - mae: 3.1989 - val_loss: 14198.8587 - val_mse: 16695.8633 - val_mae: 3.4612\n",
      "Epoch 179/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15956.4160 - mse: 9699.6465 - mae: 3.4301 - val_loss: 13678.3657 - val_mse: 14599.2676 - val_mae: 3.2958\n",
      "Epoch 180/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 13215.3892 - mse: 746.4171 - mae: 3.0669 - val_loss: 15161.4809 - val_mse: 10615.6133 - val_mae: 3.4236\n",
      "Epoch 181/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 13279.0042 - mse: 1867.1281 - mae: 3.1610 - val_loss: 15584.5293 - val_mse: 24516.2695 - val_mae: 3.6291\n",
      "Epoch 182/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14502.2536 - mse: 6403.7749 - mae: 3.2249 - val_loss: 13603.2905 - val_mse: 35470.2852 - val_mae: 3.3632\n",
      "Epoch 183/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 13974.1499 - mse: 1047.1279 - mae: 3.1125 - val_loss: 18063.6234 - val_mse: 122988.3750 - val_mae: 3.8897\n",
      "Epoch 184/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 13929.1413 - mse: 590.3221 - mae: 3.0718 - val_loss: 14888.7501 - val_mse: 107726.2109 - val_mae: 3.6655\n",
      "Epoch 185/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 15753.0667 - mse: 1201.5449 - mae: 3.2795 - val_loss: 14127.0796 - val_mse: 8572.1572 - val_mae: 3.2964\n",
      "Epoch 186/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 16816.9546 - mse: 52625.7148 - mae: 3.7627 - val_loss: 19537.4216 - val_mse: 439097.2812 - val_mae: 4.7734\n",
      "Epoch 187/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 14142.6160 - mse: 596.4321 - mae: 3.0976 - val_loss: 12658.8811 - val_mse: 33390.0469 - val_mae: 3.1235\n",
      "Epoch 188/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14541.7824 - mse: 7599.3838 - mae: 3.2966 - val_loss: 22202.5604 - val_mse: 409849.0000 - val_mae: 4.7750\n",
      "Epoch 189/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14719.7970 - mse: 11933.5518 - mae: 3.1656 - val_loss: 18386.4205 - val_mse: 362328.5625 - val_mae: 4.5721\n",
      "Epoch 190/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 13823.8625 - mse: 938.0981 - mae: 3.0699 - val_loss: 15374.8639 - val_mse: 261520.0469 - val_mae: 4.0242\n",
      "Epoch 191/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 15843.5610 - mse: 26009.3945 - mae: 3.4967 - val_loss: 15153.6921 - val_mse: 6070.7646 - val_mae: 3.3401\n",
      "Epoch 192/200\n",
      "94/94 [==============================] - 12s 123ms/step - loss: 14444.1573 - mse: 919.7583 - mae: 3.0642 - val_loss: 13524.3207 - val_mse: 16317.5654 - val_mae: 3.0098\n",
      "Epoch 193/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 14319.7176 - mse: 789.8100 - mae: 3.1465 - val_loss: 12363.3946 - val_mse: 4351.8481 - val_mae: 2.9961\n",
      "Epoch 194/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 12644.0565 - mse: 524.8552 - mae: 2.9864 - val_loss: 12099.1620 - val_mse: 1239.7349 - val_mae: 2.8960\n",
      "Epoch 195/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 12815.1178 - mse: 620.4475 - mae: 2.9500 - val_loss: 12574.5924 - val_mse: 1599.5670 - val_mae: 2.9759\n",
      "Epoch 196/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 14525.4816 - mse: 407.8054 - mae: 3.1246 - val_loss: 13195.8174 - val_mse: 6999.8711 - val_mae: 3.0892\n",
      "Epoch 197/200\n",
      "94/94 [==============================] - 12s 126ms/step - loss: 16034.8351 - mse: 153323.0469 - mae: 3.7059 - val_loss: 17103.9536 - val_mse: 282439.7812 - val_mae: 4.2393\n",
      "Epoch 198/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 13357.4096 - mse: 1973.6947 - mae: 3.0677 - val_loss: 17528.7376 - val_mse: 1936966.1250 - val_mae: 4.7706\n",
      "Epoch 199/200\n",
      "94/94 [==============================] - 12s 124ms/step - loss: 12808.4668 - mse: 431.8689 - mae: 2.9533 - val_loss: 18493.7555 - val_mse: 1442264.7500 - val_mae: 4.9861\n",
      "Epoch 200/200\n",
      "94/94 [==============================] - 12s 125ms/step - loss: 13750.5120 - mse: 295.5632 - mae: 3.0282 - val_loss: 13677.3165 - val_mse: 32124.9473 - val_mae: 3.3367\n"
     ]
    }
   ],
   "source": [
    "t.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (3.1.3)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from matplotlib) (1.18.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from matplotlib) (2.4.6)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from matplotlib) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from matplotlib) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: setuptools in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (46.1.3.post20200330)\r\n",
      "Requirement already satisfied: six>=1.5 in /home_ext/ios/anaconda3/envs/DeepRain/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\r\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "history = t.history\n",
    "def plotHistory():\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history['mse'])\n",
    "    plt.plot(history['val_mse'])\n",
    "    plt.title('Model MSE')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "plotHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = t.model\n",
    "pred = None\n",
    "label = None\n",
    "pred = 10\n",
    "test.on_epoch_end()\n",
    "mean = []\n",
    "stdev = []\n",
    "label = []\n",
    "for x,y in test:\n",
    "    for i in range(batch_size):\n",
    "        if y[i,:,:,:].max() > 0:\n",
    "            pred = model(np.array([x[i,:,:,:]]))\n",
    "            mean.append(pred.mean())\n",
    "            stdev.append(pred.stddev())\n",
    "            label.append(y[i,:,:,:])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 3, figsize=(16,8*10),dpi=64)\n",
    "#fig.set_title([\"mean\",\"stdev\",\"label\"])\n",
    "for batch,img in enumerate(mean):\n",
    "    if batch == 10:\n",
    "        break\n",
    "    axes[batch,0].imshow(img[0,:,:,0],cmap=\"gray\")\n",
    "    axes[batch,1].imshow(stdev[batch][0,:,:,0],cmap=\"gray\")\n",
    "    axes[batch,2].imshow(label[batch][:,:,0],cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
